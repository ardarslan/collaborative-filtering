{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_by_backprop = False\n",
    "random_state = 42\n",
    "import os\n",
    "import numpy as np; np.random.seed(random_state)\n",
    "import torch; torch.manual_seed(random_state)\n",
    "import random; random.seed(random_state)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_pd = pd.read_csv('../../data/data_train.csv')\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 100\n",
    "show_validation_score_every_epochs = 1\n",
    "embedding_size = 200\n",
    "learning_rate = 7e-4\n",
    "l1_reg = 1e-5\n",
    "l2_reg = 0\n",
    "mean_init = 0.2\n",
    "std_init = 0.001\n",
    "train_size = 0.9\n",
    "batch_size = 64\n",
    "num_workers = 5 # data_loader\n",
    "\n",
    "if bayes_by_backprop:\n",
    "    logsigma_constant_init = -4.6 # -3.9 # -6.90725523732\n",
    "    prior_mu = 0.0 # 0.2\n",
    "    prior_sigma = 0.01\n",
    "    mu_mean_init = 0.0\n",
    "    mu_std_init = 0.01\n",
    "    KL_coeff = 1/600000\n",
    "\n",
    "train_pd, test_pd = train_test_split(data_pd, train_size=train_size, random_state=random_state)\n",
    "\n",
    "def extract_users_items_labels(data_pd):\n",
    "    users, movies = \\\n",
    "        [np.squeeze(arr) for arr in np.split(data_pd.Id.str.extract('r(\\d+)_c(\\d+)').values.astype(int) - 1, 2, axis=-1)]\n",
    "    labels = data_pd.Prediction.values\n",
    "    return users, movies, labels\n",
    "\n",
    "train_users, train_movies, train_labels = extract_users_items_labels(train_pd)\n",
    "test_users, test_movies, test_labels = extract_users_items_labels(test_pd)\n",
    "\n",
    "movies_rated_by_user_u = {}\n",
    "for train_user, train_movie in zip(train_users, train_movies):\n",
    "    if train_user in movies_rated_by_user_u.keys():\n",
    "        movies_rated_by_user_u[train_user].append(train_movie + 1)\n",
    "    else:\n",
    "        movies_rated_by_user_u[train_user] = [train_movie + 1]\n",
    "largest_number_of_ratings_per_user = max(len(movies) for user, movies in movies_rated_by_user_u.items())\n",
    "\n",
    "users_who_rated_movie_i = {}\n",
    "for train_user, train_movie in zip(train_users, train_movies):\n",
    "    if train_movie in users_who_rated_movie_i.keys():\n",
    "        users_who_rated_movie_i[train_movie].append(train_user + 1)\n",
    "    else:\n",
    "        users_who_rated_movie_i[train_movie] = [train_user + 1]\n",
    "largest_number_of_ratings_per_movie = max(len(users) for movie, users in users_who_rated_movie_i.items())\n",
    "\n",
    "sqrt_of_number_of_movies_rated_by_user_u = dict((user, np.sqrt(len(movies))) for user, movies in movies_rated_by_user_u.items())\n",
    "sqrt_of_number_of_users_who_rated_movie_i = dict((movie, np.sqrt(len(users))) for movie, users in users_who_rated_movie_i.items())\n",
    "\n",
    "is_known_user = dict((user, 0.0) for user in test_users)\n",
    "for train_user in train_users:\n",
    "    is_known_user[train_user] = 1.0\n",
    "\n",
    "is_known_movie = dict((movie, 0.0) for movie in test_movies)\n",
    "for train_movie in train_movies:\n",
    "    is_known_movie[train_movie] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 users,\n",
    "                 movies,\n",
    "                 labels,\n",
    "                 movies_rated_by_user_u,\n",
    "                 users_who_rated_movie_i,\n",
    "                 sqrt_of_number_of_movies_rated_by_user_u,\n",
    "                 sqrt_of_number_of_users_who_rated_movie_i,\n",
    "                 is_known_user,\n",
    "                 is_known_movie):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.labels = labels\n",
    "        self.movies_rated_by_user_u = movies_rated_by_user_u\n",
    "        self.users_who_rated_movie_i = users_who_rated_movie_i\n",
    "        self.sqrt_of_number_of_movies_rated_by_user_u = sqrt_of_number_of_movies_rated_by_user_u\n",
    "        self.sqrt_of_number_of_users_who_rated_movie_i = sqrt_of_number_of_users_who_rated_movie_i\n",
    "        self.is_known_user = is_known_user\n",
    "        self.is_known_movie = is_known_movie\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_user = self.users[idx]\n",
    "        current_movie = self.movies[idx]\n",
    "        data_out = {\n",
    "            \"user\": np.array([self.users[idx]]),\n",
    "            \"movie\": np.array([self.movies[idx]]),\n",
    "            \"movies_rated_by_this_user\": np.array(self.movies_rated_by_user_u[current_user]),\n",
    "            \"users_who_rated_this_movie\": np.array(self.users_who_rated_movie_i[current_movie]),\n",
    "            \"sqrt_of_number_of_movies_rated_by_this_user\": np.array([self.sqrt_of_number_of_movies_rated_by_user_u[current_user]]),\n",
    "            \"sqrt_of_number_of_users_who_rated_this_movie\": np.array([self.sqrt_of_number_of_users_who_rated_movie_i[current_movie]]),\n",
    "            \"is_known_user\": np.array([self.is_known_user[current_user]]),\n",
    "            \"is_known_movie\": np.array([self.is_known_movie[current_movie]]),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            data_out[\"label\"] = self.labels[idx]\n",
    "\n",
    "        data_out.update(\n",
    "            {\n",
    "                key: val.astype(np.float32)\n",
    "                for key, val in data_out.items()\n",
    "                if isinstance(val, np.ndarray) and val.dtype == np.float64\n",
    "            }\n",
    "        )\n",
    "        return data_out\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        batch = []\n",
    "        for sample in data:\n",
    "            sample[\"movies_rated_by_this_user\"].resize(largest_number_of_ratings_per_user)\n",
    "            sample[\"users_who_rated_this_movie\"].resize(largest_number_of_ratings_per_movie)\n",
    "            batch.append(sample)\n",
    "        batch = default_collate(batch)\n",
    "        return batch\n",
    "\n",
    "train_dataset = MyDataset(\n",
    "    users=train_users,\n",
    "    movies=train_movies,\n",
    "    labels=train_labels,\n",
    "    movies_rated_by_user_u=movies_rated_by_user_u,\n",
    "    users_who_rated_movie_i=users_who_rated_movie_i,\n",
    "    sqrt_of_number_of_movies_rated_by_user_u=sqrt_of_number_of_movies_rated_by_user_u,\n",
    "    sqrt_of_number_of_users_who_rated_movie_i=sqrt_of_number_of_users_who_rated_movie_i,\n",
    "    is_known_user=is_known_user,\n",
    "    is_known_movie=is_known_movie\n",
    ")\n",
    "\n",
    "test_dataset = MyDataset(\n",
    "    users=test_users,\n",
    "    movies=test_movies,\n",
    "    labels=test_labels,\n",
    "    movies_rated_by_user_u=movies_rated_by_user_u,\n",
    "    users_who_rated_movie_i=users_who_rated_movie_i,\n",
    "    sqrt_of_number_of_movies_rated_by_user_u=sqrt_of_number_of_movies_rated_by_user_u,\n",
    "    sqrt_of_number_of_users_who_rated_movie_i=sqrt_of_number_of_users_who_rated_movie_i,\n",
    "    is_known_user=is_known_user,\n",
    "    is_known_movie=is_known_movie\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.collate_fn,\n",
    "    num_workers=num_workers)\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_dataset.collate_fn,\n",
    "    num_workers=num_workers)\n",
    "\n",
    "def data_2_device(data, device):\n",
    "    for key in data.keys():\n",
    "        if torch.is_tensor(data[key]):\n",
    "            data[key] = data[key].to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "import numpy as np\n",
    "np.random.seed(random_state)\n",
    "import torch\n",
    "torch.manual_seed(random_state)\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)\n",
    "\n",
    "number_of_users, number_of_movies = (10000, 1000)\n",
    "\n",
    "class SVDPP(nn.Module):\n",
    "    def __init__(self, number_of_users, number_of_movies, embedding_size, global_mean, mean_init, std_init):\n",
    "        super().__init__()\n",
    "        self.global_mean = global_mean\n",
    "        self.Bu = nn.Embedding(number_of_users, 1)\n",
    "        nn.init.normal_(self.Bu.weight, mean=mean_init, std=std_init)\n",
    "        self.Bi = nn.Embedding(number_of_movies, 1)\n",
    "        nn.init.normal_(self.Bu.weight, mean=mean_init, std=std_init)\n",
    "        self.P = nn.Embedding(number_of_users, embedding_size)\n",
    "        nn.init.normal_(self.P.weight, mean=mean_init, std=std_init)\n",
    "        self.Q = nn.Embedding(number_of_movies, embedding_size)\n",
    "        nn.init.normal_(self.Q.weight, mean=mean_init, std=std_init)\n",
    "        self.Y = nn.Embedding(number_of_movies + 1, embedding_size, padding_idx=0) # Made this 1-indexed to save memory in GPU. (To pad movies_rated_by_this_user with zeros.)\n",
    "        nn.init.normal_(self.Y.weight, mean=mean_init, std=std_init)\n",
    "        # self.Z = nn.Embedding(number_of_users + 1, embedding_size, padding_idx=0) # Made this 1-indexed to save memory in GPU. (To pad movies_rated_by_this_user with zeros.)\n",
    "        # nn.init.normal_(self.Z.weight, mean=mean_init, std=std_init)\n",
    "\n",
    "    def forward(self, data):\n",
    "        users, movies, movies_rated_by_this_user, users_who_rated_this_movie, sqrt_of_number_of_movies_rated_by_this_user, sqrt_of_number_of_users_who_rated_this_movie, is_known_user, is_known_movie = torch.squeeze(data[\"user\"]), torch.squeeze(data[\"movie\"]), data[\"movies_rated_by_this_user\"], data[\"users_who_rated_this_movie\"], data[\"sqrt_of_number_of_movies_rated_by_this_user\"], data[\"sqrt_of_number_of_users_who_rated_this_movie\"], data[\"is_known_user\"], data[\"is_known_movie\"]\n",
    "        gm = self.global_mean\n",
    "\n",
    "        bu = self.Bu(users)\n",
    "        if not self.training:\n",
    "            bu = is_known_user * bu\n",
    "\n",
    "        bi = self.Bi(movies)\n",
    "        if not self.training:\n",
    "            bi = is_known_movie * bi\n",
    "\n",
    "        p = self.P(users)\n",
    "        if not self.training:\n",
    "            p = is_known_user * p\n",
    "\n",
    "        q = self.Q(movies)\n",
    "        if not self.training:\n",
    "            q = is_known_movie * q\n",
    "\n",
    "        y = self.Y(movies_rated_by_this_user).sum(dim=1).div(sqrt_of_number_of_movies_rated_by_this_user)\n",
    "        if not self.training:\n",
    "            y = is_known_user * y\n",
    "            \n",
    "        # z = self.Z(users_who_rated_this_movie).sum(dim=1).div(sqrt_of_number_of_users_who_rated_this_movie)\n",
    "        # if not self.training:\n",
    "        #     z = is_known_movie * z\n",
    "\n",
    "        result = q.mul(p+y).sum(dim=1) + torch.squeeze(bi) + torch.squeeze(bu) + gm\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "class SVDPP_Bayes_by_Backprop(nn.Module):\n",
    "    def __init__(self,\n",
    "                 number_of_users,\n",
    "                 number_of_movies,\n",
    "                 embedding_size,\n",
    "                 global_mean,\n",
    "                 prior_mu,\n",
    "                 prior_sigma,\n",
    "                 mu_mean_init,\n",
    "                 mu_std_init,\n",
    "                 logsigma_constant_init):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.prior_mu = prior_mu\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.global_mean = global_mean\n",
    "        \n",
    "        self.Bu_mu = nn.Embedding(number_of_users, 1)\n",
    "        nn.init.normal_(self.Bu_mu.weight, mean=mu_mean_init, std=mu_std_init)\n",
    "        self.Bu_logsigma = nn.Embedding(number_of_users, 1)\n",
    "        nn.init.constant_(self.Bu_logsigma.weight, logsigma_constant_init)\n",
    "        \n",
    "        self.Bi_mu = nn.Embedding(number_of_movies, 1)\n",
    "        nn.init.normal_(self.Bi_mu.weight, mean=mu_mean_init, std=mu_std_init)\n",
    "        self.Bi_logsigma = nn.Embedding(number_of_movies, 1)\n",
    "        nn.init.constant_(self.Bi_logsigma.weight, logsigma_constant_init)\n",
    "        \n",
    "        self.P_mu = nn.Embedding(number_of_users, embedding_size)\n",
    "        nn.init.normal_(self.P_mu.weight, mean=mu_mean_init, std=mu_std_init)\n",
    "        self.P_logsigma = nn.Embedding(number_of_users, embedding_size)\n",
    "        nn.init.constant_(self.P_logsigma.weight, logsigma_constant_init)\n",
    "        \n",
    "        self.Q_mu = nn.Embedding(number_of_movies, embedding_size)\n",
    "        nn.init.normal_(self.Q_mu.weight, mean=mu_mean_init, std=mu_std_init)\n",
    "        self.Q_logsigma = nn.Embedding(number_of_movies, embedding_size)\n",
    "        nn.init.constant_(self.Q_logsigma.weight, logsigma_constant_init)\n",
    "        \n",
    "        self.Y_mu = nn.Embedding(number_of_movies + 1, embedding_size, padding_idx=0) # Made this 1-indexed to save memory in GPU.\n",
    "        nn.init.normal_(self.Y_mu.weight, mean=mu_mean_init, std=mu_std_init)\n",
    "        self.Y_logsigma = nn.Embedding(number_of_movies + 1, embedding_size, padding_idx=0) # Made this 1-indexed to save memory in GPU.\n",
    "        nn.init.constant_(self.Y_logsigma.weight, logsigma_constant_init)\n",
    "        \n",
    "        # self.Z_mu = nn.Embedding(number_of_users + 1, embedding_size, padding_idx=0) # Made this 1-indexed to save memory in GPU.\n",
    "        # nn.init.normal_(self.Z_mu.weight, mean=mu_mean_init, std=mu_std_init)\n",
    "        # self.Z_logsigma = nn.Embedding(number_of_users + 1, embedding_size, padding_idx=0) # Made this 1-indexed to save memory in GPU.\n",
    "        # nn.init.constant_(self.Z_logsigma.weight, logsigma_constant_init)\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        users, movies, movies_rated_by_this_user, users_who_rated_this_movie, sqrt_of_number_of_movies_rated_by_this_user, sqrt_of_number_of_users_who_rated_this_movie, is_known_user, is_known_movie = torch.squeeze(data[\"user\"]), torch.squeeze(data[\"movie\"]), data[\"movies_rated_by_this_user\"], data[\"users_who_rated_this_movie\"], data[\"sqrt_of_number_of_movies_rated_by_this_user\"], data[\"sqrt_of_number_of_users_who_rated_this_movie\"], data[\"is_known_user\"], data[\"is_known_movie\"]\n",
    "        gm = self.global_mean\n",
    "\n",
    "        bu_mu = self.Bu_mu(users)\n",
    "        bu_sigma = F.softplus(self.Bu_logsigma(users))\n",
    "        bu = bu_mu + bu_sigma * torch.normal(mean=torch.zeros_like(bu_mu), std=torch.ones_like(bu_mu))\n",
    "        if not self.training:\n",
    "            bu = is_known_user * bu\n",
    "\n",
    "        bi_mu = self.Bi_mu(movies)\n",
    "        bi_sigma = F.softplus(self.Bi_logsigma(movies))\n",
    "        bi = bi_mu + bi_sigma * torch.normal(mean=torch.zeros_like(bi_mu), std=torch.ones_like(bi_mu))\n",
    "        if not self.training:\n",
    "            bi = is_known_movie * bi\n",
    "\n",
    "        p_mu = self.P_mu(users)\n",
    "        p_sigma = F.softplus(self.P_logsigma(users))\n",
    "        p = p_mu + p_sigma * torch.normal(mean=torch.zeros_like(p_mu), std=torch.ones_like(p_mu))\n",
    "        if not self.training:\n",
    "            p = is_known_user * p\n",
    "\n",
    "        q_mu = self.Q_mu(movies)\n",
    "        q_sigma = F.softplus(self.Q_logsigma(movies))\n",
    "        q = q_mu + q_sigma * torch.normal(mean=torch.zeros_like(q_mu), std=torch.ones_like(q_mu))\n",
    "        if not self.training:\n",
    "            q = is_known_movie * q\n",
    "\n",
    "        y_mu = self.Y_mu(movies_rated_by_this_user)\n",
    "        y_sigma = F.softplus(self.Y_logsigma(movies_rated_by_this_user))\n",
    "        y = y_mu + y_sigma * torch.normal(mean=torch.zeros_like(y_mu), std=torch.ones_like(y_mu))\n",
    "        y = y.sum(dim=1).div(sqrt_of_number_of_movies_rated_by_this_user)\n",
    "        if not self.training:\n",
    "            y = is_known_user * y\n",
    "            \n",
    "        # z_mu = self.Z_mu(users_who_rated_this_movie)\n",
    "        # z_sigma = F.softplus(self.Z_logsigma(users_who_rated_this_movie))\n",
    "        # z = z_mu + z_sigma * torch.normal(mean=torch.zeros_like(z_mu), std=torch.ones_like(z_mu))\n",
    "        # z = z.sum(dim=1).div(sqrt_of_number_of_users_who_rated_this_movie)\n",
    "        # if not self.training:\n",
    "        #     z = is_known_movie * z\n",
    "\n",
    "        result = q.mul(p+y).sum(dim=1) + torch.squeeze(bi) + torch.squeeze(bu) + gm\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        '''\n",
    "        Computes the KL divergence between the priors and posteriors of all embeddings.\n",
    "        '''\n",
    "        kl_loss = self._kl_divergence(self.Bu_mu.weight, self.Bu_logsigma.weight)\n",
    "        kl_loss += self._kl_divergence(self.Bi_mu.weight, self.Bi_logsigma.weight)\n",
    "        kl_loss += self._kl_divergence(self.P_mu.weight, self.P_logsigma.weight)\n",
    "        kl_loss += self._kl_divergence(self.Q_mu.weight, self.Q_logsigma.weight)\n",
    "        kl_loss += self._kl_divergence(self.Y_mu.weight, self.Y_logsigma.weight)\n",
    "        return kl_loss\n",
    "\n",
    "    def _kl_divergence(self, mu, logsigma):\n",
    "        '''\n",
    "        Computes the KL divergence between one Gaussian posterior\n",
    "        and the Gaussian prior.\n",
    "        '''\n",
    "        sigma = F.softplus(logsigma)\n",
    "        params = mu + sigma * torch.normal(mean=torch.zeros_like(mu), std=torch.ones_like(mu))\n",
    "        \n",
    "        p_prior_dist = torch.distributions.normal.Normal(self.prior_mu, self.prior_sigma)\n",
    "        p_prior_log_prob = p_prior_dist.log_prob(params)\n",
    "        \n",
    "        q_posterior_dist = torch.distributions.normal.Normal(mu, sigma)\n",
    "        q_posterior_log_prob = q_posterior_dist.log_prob(params)\n",
    "        \n",
    "        kl = torch.sum(q_posterior_log_prob - p_prior_log_prob)\n",
    "\n",
    "        return kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = lambda x, y: np.sqrt(mean_squared_error(x, y))\n",
    "\n",
    "def mse_loss(predictions, labels):\n",
    "    return torch.mean((predictions - labels) ** 2)\n",
    "\n",
    "global_mean = np.mean(train_labels)\n",
    "\n",
    "if bayes_by_backprop:\n",
    "    model = SVDPP_Bayes_by_Backprop(number_of_users,\n",
    "                                    number_of_movies,\n",
    "                                    embedding_size,\n",
    "                                    global_mean,\n",
    "                                    prior_mu,\n",
    "                                    prior_sigma,\n",
    "                                    mu_mean_init,\n",
    "                                    mu_std_init,\n",
    "                                    logsigma_constant_init).float().to(device)\n",
    "else:\n",
    "    model = SVDPP(number_of_users,\n",
    "                  number_of_movies,\n",
    "                  embedding_size,\n",
    "                  global_mean,\n",
    "                  mean_init,\n",
    "                  std_init).float().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=l2_reg)\n",
    "\n",
    "train_rmse_values = []\n",
    "test_rmse_values = []\n",
    "step = 0\n",
    "with tqdm(total=len(train_data_loader) * num_epochs) as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            data_2_device(batch, device)\n",
    "\n",
    "            predictions_batch = model(batch)\n",
    "            labels_batch = batch[\"label\"]\n",
    "\n",
    "            loss = mse_loss(predictions_batch, labels_batch.float())\n",
    "            \n",
    "            for embedding_layer in [model.Bu, model.Bi, model.P, model.Q, model.Y]:\n",
    "                loss += l1_reg * torch.norm(embedding_layer.weight, 1)\n",
    "            \n",
    "            if bayes_by_backprop:\n",
    "                loss += (KL_coeff * model.kl_divergence() / batch_size)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        if epoch % show_validation_score_every_epochs == 0:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                all_train_predictions = []\n",
    "                all_train_labels = []\n",
    "                for batch in train_data_loader:\n",
    "                    data_2_device(batch, device)\n",
    "                    predictions_batch = model(batch)\n",
    "                    labels_batch = batch[\"label\"]\n",
    "                    all_train_predictions.extend(predictions_batch.detach().cpu().numpy().ravel().tolist())\n",
    "                    all_train_labels.extend(labels_batch.cpu().numpy().ravel().tolist())\n",
    "\n",
    "                all_test_predictions = []\n",
    "                all_test_labels = []\n",
    "                for batch in test_data_loader:\n",
    "                    data_2_device(batch, device)\n",
    "                    predictions_batch = model(batch)\n",
    "                    labels_batch = batch[\"label\"]\n",
    "                    all_test_predictions.extend(predictions_batch.detach().cpu().numpy().ravel().tolist())\n",
    "                    all_test_labels.extend(labels_batch.cpu().numpy().ravel().tolist())\n",
    "\n",
    "            all_train_labels = np.array(all_train_labels)\n",
    "            all_test_labels = np.array(all_test_labels)\n",
    "            all_train_predictions = np.clip(np.array(all_train_predictions), 1, 5)\n",
    "            all_test_predictions = np.clip(np.array(all_test_predictions), 1, 5)\n",
    "            train_rmse = rmse(all_train_labels, all_train_predictions)\n",
    "            test_rmse = rmse(all_test_labels, all_test_predictions)\n",
    "            print('Epoch: {:3d}, Train RMSE: {:.4f}, Test RMSE: {:.4f}'.format(epoch, train_rmse, test_rmse))\n",
    "            train_rmse_values.append(train_rmse)\n",
    "            test_rmse_values.append(test_rmse)\n",
    "\n",
    "            model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use different regularization techniques other than L2. e.g. set prior on latent vectors and add KL divergence to loss\n",
    "\n",
    "2) Make use of nonlinearity\n",
    "\n",
    "3) Instead of dividing by np.sqrt(len(number_of_rates)), learn this function\n",
    "\n",
    "4) Add additional features: Number of movies this user watched, number of users this movie was watched by, clustering features, each user's frequencies for different ratings, each movies frequencies for different ratings\n",
    "\n",
    "5) Add implicit features for movies.\n",
    "\n",
    "6) Try classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
