{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "number_of_users, number_of_movies = (10000, 1000)\n",
    "\n",
    "data_pd = pd.read_csv('../../data/data_train.csv')\n",
    "\n",
    "train_size = 0.9\n",
    "\n",
    "train_pd, test_pd = train_test_split(data_pd, train_size=train_size, random_state=42)\n",
    "\n",
    "def extract_users_items_labels(data_pd):\n",
    "    users, movies = \\\n",
    "        [np.squeeze(arr) for arr in np.split(data_pd.Id.str.extract('r(\\d+)_c(\\d+)').values.astype(int) - 1, 2, axis=-1)]\n",
    "    labels = data_pd.Prediction.values\n",
    "    return users, movies, labels\n",
    "\n",
    "train_users, train_movies, train_labels = extract_users_items_labels(train_pd)\n",
    "\n",
    "movies_rated_by_user_u = {}\n",
    "for train_user, train_movie in zip(train_users, train_movies):\n",
    "    if train_user in movies_rated_by_user_u.keys():\n",
    "        movies_rated_by_user_u[train_user].append(train_movie)\n",
    "    else:\n",
    "        movies_rated_by_user_u[train_user] = [train_movie]\n",
    "\n",
    "test_users, test_movies, test_labels = extract_users_items_labels(test_pd)\n",
    "\n",
    "rmse = lambda x, y: math.sqrt(mean_squared_error(x, y))\n",
    "\n",
    "def extract_prediction_from_full_matrix(reconstructed_matrix, users=test_users, movies=test_movies):\n",
    "    # returns predictions for the users-movies combinations specified based on a full m \\times n matrix\n",
    "    assert(len(users) == len(movies)), \"users-movies combinations specified should have equal length\"\n",
    "    predictions = np.zeros(len(test_users))\n",
    "\n",
    "    for i, (user, movie) in enumerate(zip(users, movies)):\n",
    "        predictions[i] = reconstructed_matrix[user][movie]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "show_validation_score_every_epochs = 1\n",
    "embedding_size = 200\n",
    "learning_rate = 7e-4\n",
    "weight_decay = 7e-5\n",
    "mean_init = 0.2\n",
    "std_init = 0.001\n",
    "\n",
    "class SVDPP(nn.Module):\n",
    "    def __init__(self, number_of_users, number_of_movies, embedding_size, global_mean, is_train_user_arr, is_train_movie_arr, movies_rated_by_user_u, mean_init, std_init):\n",
    "        super().__init__()\n",
    "        self.Bu = nn.Embedding(number_of_users, 1)\n",
    "        nn.init.normal_(self.Bu.weight, mean=mean_init, std=std_init)\n",
    "        self.Bi = nn.Embedding(number_of_movies, 1)\n",
    "        nn.init.normal_(self.Bu.weight, mean=mean_init, std=std_init)\n",
    "        self.P = nn.Embedding(number_of_users, embedding_size)\n",
    "        nn.init.normal_(self.P.weight, mean=mean_init, std=std_init)\n",
    "        self.Q = nn.Embedding(number_of_movies, embedding_size)\n",
    "        nn.init.normal_(self.Q.weight, mean=mean_init, std=std_init)\n",
    "        self.Y = nn.Embedding(number_of_movies, embedding_size)\n",
    "        nn.init.normal_(self.Y.weight, mean=mean_init, std=std_init)\n",
    "        self.global_mean = torch.tensor(global_mean, requires_grad=False)\n",
    "        self.mask_unknown_users = nn.Embedding.from_pretrained(torch.FloatTensor(is_train_user_arr), freeze=True)\n",
    "        self.mask_unknown_movies = nn.Embedding.from_pretrained(torch.FloatTensor(is_train_movie_arr), freeze=True)\n",
    "        self.movies_rated_by_user_u = movies_rated_by_user_u\n",
    "\n",
    "    def get_y(self, users):\n",
    "        js_list = [self.movies_rated_by_user_u[u] for u in users.cpu().numpy()]\n",
    "        js_lengths = [len(js) for js in js_list]\n",
    "        js_list_concatted = np.hstack(tuple(js_list))\n",
    "        temp = self.Y(torch.tensor(js_list_concatted, device=device))\n",
    "        last_index = 0\n",
    "        y = []\n",
    "        for js_length in js_lengths:\n",
    "            current_tensor = temp[last_index:last_index+js_length, :].sum(dim=0)\n",
    "            current_tensor = current_tensor.div(np.sqrt(js_length))\n",
    "            y.append(current_tensor)\n",
    "            last_index += js_length\n",
    "        y = torch.stack(y, dim=0)\n",
    "        return y\n",
    "\n",
    "    def forward(self, users, movies):\n",
    "        if self.training:\n",
    "            bu = self.Bu(users)\n",
    "            bi = self.Bi(movies)\n",
    "            gm = self.global_mean\n",
    "            p = self.P(users)\n",
    "            q = self.Q(movies)\n",
    "            y = self.get_y(users)\n",
    "            result = q.mul(p+y).sum(dim=1) + gm + torch.squeeze(bi) + torch.squeeze(bu)\n",
    "            return result\n",
    "        else:\n",
    "            users_mask = self.mask_unknown_users(users)\n",
    "            movies_mask = self.mask_unknown_movies(movies)\n",
    "            bu = users_mask * self.Bu(users)\n",
    "            bi = movies_mask * self.Bi(movies)\n",
    "            gm = self.global_mean\n",
    "            p = users_mask * self.P(users)\n",
    "            q = movies_mask * self.Q(movies)\n",
    "            y = movies_mask * users_mask * self.get_y(users)\n",
    "            result = q.mul(p+y).sum(dim=1) + gm + torch.squeeze(bi) + torch.squeeze(bu)\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4907f368ee3493d91708fc82aabbf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1655100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0, Train RMSE: 1.0238, Test RMSE: 1.0326\n",
      "Epoch:   1, Train RMSE: 0.9782, Test RMSE: 0.9966\n",
      "Epoch:   2, Train RMSE: 0.9659, Test RMSE: 0.9901\n",
      "Epoch:   3, Train RMSE: 0.9585, Test RMSE: 0.9872\n",
      "Epoch:   4, Train RMSE: 0.9535, Test RMSE: 0.9853\n",
      "Epoch:   5, Train RMSE: 0.9497, Test RMSE: 0.9839\n",
      "Epoch:   6, Train RMSE: 0.9468, Test RMSE: 0.9830\n",
      "Epoch:   7, Train RMSE: 0.9445, Test RMSE: 0.9823\n",
      "Epoch:   8, Train RMSE: 0.9426, Test RMSE: 0.9819\n",
      "Epoch:   9, Train RMSE: 0.9411, Test RMSE: 0.9816\n",
      "Epoch:  10, Train RMSE: 0.9396, Test RMSE: 0.9814\n",
      "Epoch:  11, Train RMSE: 0.9383, Test RMSE: 0.9812\n",
      "Epoch:  12, Train RMSE: 0.9370, Test RMSE: 0.9810\n",
      "Epoch:  13, Train RMSE: 0.9360, Test RMSE: 0.9810\n",
      "Epoch:  14, Train RMSE: 0.9351, Test RMSE: 0.9809\n",
      "Epoch:  15, Train RMSE: 0.9344, Test RMSE: 0.9809\n",
      "Epoch:  16, Train RMSE: 0.9337, Test RMSE: 0.9809\n",
      "Epoch:  17, Train RMSE: 0.9331, Test RMSE: 0.9808\n",
      "Epoch:  18, Train RMSE: 0.9326, Test RMSE: 0.9808\n",
      "Epoch:  19, Train RMSE: 0.9324, Test RMSE: 0.9808\n",
      "Epoch:  20, Train RMSE: 0.9320, Test RMSE: 0.9807\n",
      "Epoch:  21, Train RMSE: 0.9317, Test RMSE: 0.9808\n",
      "Epoch:  22, Train RMSE: 0.9314, Test RMSE: 0.9807\n",
      "Epoch:  23, Train RMSE: 0.9312, Test RMSE: 0.9807\n",
      "Epoch:  24, Train RMSE: 0.9311, Test RMSE: 0.9808\n",
      "Epoch:  25, Train RMSE: 0.9309, Test RMSE: 0.9807\n",
      "Epoch:  26, Train RMSE: 0.9308, Test RMSE: 0.9808\n",
      "Epoch:  27, Train RMSE: 0.9308, Test RMSE: 0.9808\n",
      "Epoch:  28, Train RMSE: 0.9307, Test RMSE: 0.9808\n",
      "Epoch:  29, Train RMSE: 0.9306, Test RMSE: 0.9809\n",
      "Epoch:  30, Train RMSE: 0.9305, Test RMSE: 0.9809\n",
      "Epoch:  31, Train RMSE: 0.9304, Test RMSE: 0.9809\n",
      "Epoch:  32, Train RMSE: 0.9304, Test RMSE: 0.9809\n",
      "Epoch:  33, Train RMSE: 0.9302, Test RMSE: 0.9809\n",
      "Epoch:  34, Train RMSE: 0.9301, Test RMSE: 0.9809\n",
      "Epoch:  35, Train RMSE: 0.9301, Test RMSE: 0.9809\n",
      "Epoch:  36, Train RMSE: 0.9301, Test RMSE: 0.9809\n",
      "Epoch:  37, Train RMSE: 0.9299, Test RMSE: 0.9807\n",
      "Epoch:  38, Train RMSE: 0.9298, Test RMSE: 0.9808\n",
      "Epoch:  39, Train RMSE: 0.9297, Test RMSE: 0.9807\n",
      "Epoch:  40, Train RMSE: 0.9297, Test RMSE: 0.9807\n",
      "Epoch:  41, Train RMSE: 0.9295, Test RMSE: 0.9806\n",
      "Epoch:  42, Train RMSE: 0.9296, Test RMSE: 0.9807\n",
      "Epoch:  43, Train RMSE: 0.9296, Test RMSE: 0.9807\n",
      "Epoch:  44, Train RMSE: 0.9296, Test RMSE: 0.9807\n",
      "Epoch:  45, Train RMSE: 0.9296, Test RMSE: 0.9808\n",
      "Epoch:  46, Train RMSE: 0.9295, Test RMSE: 0.9807\n",
      "Epoch:  47, Train RMSE: 0.9295, Test RMSE: 0.9808\n",
      "Epoch:  48, Train RMSE: 0.9295, Test RMSE: 0.9808\n",
      "Epoch:  49, Train RMSE: 0.9295, Test RMSE: 0.9808\n",
      "Epoch:  50, Train RMSE: 0.9294, Test RMSE: 0.9807\n",
      "Epoch:  51, Train RMSE: 0.9296, Test RMSE: 0.9809\n",
      "Epoch:  52, Train RMSE: 0.9296, Test RMSE: 0.9810\n",
      "Epoch:  53, Train RMSE: 0.9295, Test RMSE: 0.9808\n",
      "Epoch:  54, Train RMSE: 0.9295, Test RMSE: 0.9809\n",
      "Epoch:  55, Train RMSE: 0.9295, Test RMSE: 0.9808\n",
      "Epoch:  56, Train RMSE: 0.9294, Test RMSE: 0.9808\n",
      "Epoch:  57, Train RMSE: 0.9294, Test RMSE: 0.9808\n",
      "Epoch:  58, Train RMSE: 0.9294, Test RMSE: 0.9808\n",
      "Epoch:  59, Train RMSE: 0.9293, Test RMSE: 0.9808\n",
      "Epoch:  60, Train RMSE: 0.9294, Test RMSE: 0.9809\n",
      "Epoch:  61, Train RMSE: 0.9293, Test RMSE: 0.9808\n",
      "Epoch:  62, Train RMSE: 0.9293, Test RMSE: 0.9808\n",
      "Epoch:  63, Train RMSE: 0.9293, Test RMSE: 0.9808\n",
      "Epoch:  64, Train RMSE: 0.9294, Test RMSE: 0.9808\n",
      "Epoch:  65, Train RMSE: 0.9293, Test RMSE: 0.9807\n",
      "Epoch:  66, Train RMSE: 0.9293, Test RMSE: 0.9809\n",
      "Epoch:  67, Train RMSE: 0.9294, Test RMSE: 0.9809\n",
      "Epoch:  68, Train RMSE: 0.9294, Test RMSE: 0.9808\n",
      "Epoch:  69, Train RMSE: 0.9293, Test RMSE: 0.9808\n",
      "Epoch:  70, Train RMSE: 0.9292, Test RMSE: 0.9806\n",
      "Epoch:  71, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  72, Train RMSE: 0.9291, Test RMSE: 0.9806\n",
      "Epoch:  73, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  74, Train RMSE: 0.9293, Test RMSE: 0.9808\n",
      "Epoch:  75, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  76, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  77, Train RMSE: 0.9292, Test RMSE: 0.9808\n",
      "Epoch:  78, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  79, Train RMSE: 0.9291, Test RMSE: 0.9806\n",
      "Epoch:  80, Train RMSE: 0.9291, Test RMSE: 0.9807\n",
      "Epoch:  81, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  82, Train RMSE: 0.9292, Test RMSE: 0.9808\n",
      "Epoch:  83, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  84, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  85, Train RMSE: 0.9291, Test RMSE: 0.9807\n",
      "Epoch:  86, Train RMSE: 0.9291, Test RMSE: 0.9807\n",
      "Epoch:  87, Train RMSE: 0.9291, Test RMSE: 0.9806\n",
      "Epoch:  88, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  89, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  90, Train RMSE: 0.9293, Test RMSE: 0.9807\n",
      "Epoch:  91, Train RMSE: 0.9291, Test RMSE: 0.9806\n",
      "Epoch:  92, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  93, Train RMSE: 0.9293, Test RMSE: 0.9807\n",
      "Epoch:  94, Train RMSE: 0.9291, Test RMSE: 0.9807\n",
      "Epoch:  95, Train RMSE: 0.9292, Test RMSE: 0.9807\n",
      "Epoch:  96, Train RMSE: 0.9293, Test RMSE: 0.9808\n",
      "Epoch:  97, Train RMSE: 0.9294, Test RMSE: 0.9808\n",
      "Epoch:  98, Train RMSE: 0.9293, Test RMSE: 0.9807\n",
      "Epoch:  99, Train RMSE: 0.9294, Test RMSE: 0.9808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(predictions, labels):\n",
    "    return torch.mean((predictions - labels) ** 2)\n",
    "\n",
    "# Build Dataloaders\n",
    "train_users_torch = torch.tensor(train_users, device=device)\n",
    "train_movies_torch = torch.tensor(train_movies, device=device)\n",
    "train_labels_torch = torch.tensor(train_labels, device=device)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    TensorDataset(train_users_torch, train_movies_torch, train_labels_torch),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_users_torch = torch.tensor(test_users, device=device)\n",
    "test_movies_torch = torch.tensor(test_movies, device=device)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    TensorDataset(test_users_torch, test_movies_torch),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "global_mean = np.mean(train_labels)\n",
    "\n",
    "is_train_user_arr = np.zeros((number_of_users, 1))\n",
    "is_train_user_arr[train_users, 0] = 1.0\n",
    "\n",
    "is_train_movie_arr = np.zeros((number_of_movies, 1))\n",
    "is_train_movie_arr[train_movies, 0] = 1.0\n",
    "\n",
    "model = SVDPP(number_of_users, number_of_movies, embedding_size, global_mean, is_train_user_arr, is_train_movie_arr, movies_rated_by_user_u, mean_init, std_init).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)\n",
    "\n",
    "train_rmse_values = []\n",
    "test_rmse_values = []\n",
    "step = 0\n",
    "with tqdm(total=len(train_dataloader) * num_epochs) as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        for users_batch, movies_batch, labels_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions_batch = model(users_batch, movies_batch)\n",
    "\n",
    "            loss = mse_loss(predictions_batch, labels_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        if epoch % show_validation_score_every_epochs == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                all_train_predictions = []\n",
    "                for users_batch, movies_batch, _ in train_dataloader:\n",
    "                    predictions_batch = model(users_batch, movies_batch)\n",
    "                    all_train_predictions.extend(predictions_batch.detach().cpu().numpy().tolist())\n",
    "\n",
    "                all_test_predictions = []\n",
    "                for users_batch, movies_batch in test_dataloader:\n",
    "                    predictions_batch = model(users_batch, movies_batch)\n",
    "                    all_test_predictions.extend(predictions_batch.detach().cpu().numpy().tolist())\n",
    "\n",
    "            train_rmse = rmse(train_labels, all_train_predictions)\n",
    "            test_rmse = rmse(test_labels, all_test_predictions)\n",
    "            print('Epoch: {:3d}, Train RMSE: {:.4f}, Test RMSE: {:.4f}'.format(epoch, train_rmse, test_rmse))\n",
    "            train_rmse_values.append(train_rmse)\n",
    "            test_rmse_values.append(test_rmse)\n",
    "\n",
    "            model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use different regularization techniques other than L2. e.g. set prior on latent vectors and add KL divergence to loss\n",
    "\n",
    "2) Make use of nonlinearity\n",
    "\n",
    "3) Instead of dividing by np.sqrt(len(number_of_rates)), learn this function\n",
    "\n",
    "4) Add additional features: Number of movies this user watched, number of users this movie was watched by, clustering features, each user's frequencies for different ratings, each movies frequencies for different ratings\n",
    "\n",
    "5) Add implicit features for movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
